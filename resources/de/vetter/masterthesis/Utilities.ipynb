{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Utilities\n",
    "This notebook collects the relevant part of the disjoined utility-scriptlets used during the thesis, e.g. for processing GFF-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean GFF-file\n",
    "The annotation as given by Geneious from the RNA-mapping also includes (poorly) predicted Junctions, which are of no interest here. Thus, gff-files can be cleaned with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilename = \"out/scaffolds_with_testset_september13.cleaned.gff\"\n",
    "\n",
    "with open(outfilename, \"w\") as out:\n",
    "    out.write(\"##gff-version 3\\n##source-version geneious 2020.1.2\\n\")\n",
    "    \n",
    "current = \"\"\n",
    "had_rsoi = False # had regions of interest?\n",
    "with open(\"scaffolds_with_testset_september13.gff\") as gff, open(outfilename, \"a\") as out:\n",
    "    for row, line in enumerate(gff):\n",
    "        if row < 2: # header\n",
    "            continue\n",
    "        if line.startswith(\"##sequence-region\"):\n",
    "            if had_rsoi:\n",
    "                print(\"<-\", current[17:40])\n",
    "                out.write(current)\n",
    "            current = line\n",
    "            had_rsoi = False\n",
    "        elif \"Junction\" in line:\n",
    "            continue\n",
    "        else:\n",
    "            had_rsoi = True\n",
    "            current += \"\\t\".join(line.split(\"\\t\")[:-1]) + \"\\t\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Augustus-output into convenient GFF-format\n",
    "Remove some of the unnecessary elements, use to preprocess Augustus-output for evaluation of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in [\"basic\", \n",
    "             \"base-u12d12\", \"base-u18d18\", \"base-u18d24\", \"base-u24d18\", \"base-u24d24\", \n",
    "             \"codon-u12d12\", \"codon-u18d18\", \"codon-u18d24\", \"codon-u24d18\", \"codon-u24d24\"]:\n",
    "    infile = \"september-final-benchmarks/lab-augustus-%s-on-test.out\" % case\n",
    "    outfile = infile[:-3] + \"cleaned.gff\"\n",
    "\n",
    "    retain_database = False\n",
    "\n",
    "    with open(outfile, \"w\") as out:\n",
    "        out.write(\"\")\n",
    "\n",
    "    with open(infile) as augustus, open(outfile, \"a\") as out:\n",
    "        row = 0\n",
    "        for line in augustus:\n",
    "            if line.startswith(\"#\"):\n",
    "                out.write(line)\n",
    "            elif line.startswith(\"NODE_\"):\n",
    "                contents = line.split(\"\\t\")\n",
    "                splitpoint = contents[0].rfind(\"_\")\n",
    "                start, end = contents[0][splitpoint+1:].split(\"-\")\n",
    "                contents[0] = contents[0][:splitpoint]\n",
    "                print(contents[0], start, end)\n",
    "                if contents[1] == \"database\" and not retain_database:\n",
    "                    print(\"\\t ignoring database-entry\")\n",
    "                    continue\n",
    "\n",
    "                contents[3] = str(int(contents[3]) + int(start) - 1)\n",
    "                contents[4] = str(int(contents[4]) + int(start) - 1)\n",
    "                print('\\t'.join(contents))\n",
    "                out.write(\"\\t\".join(contents))\n",
    "            else:\n",
    "                print(\"\\t\", row, \":\", line[:-1])\n",
    "            row += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert blastx-output into annotation\n",
    "\n",
    "This allows the annotation-gff file to be loaded into Geneious as its own track and to be used conveniently there. This further allows some visual exploration of the homology-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hit:\n",
    "    def __init__(self, name, frame=0, score=0, evalue=0):\n",
    "        self.start_index = None\n",
    "        self.end_index = -1 # last match to query (this is already w.r.t. nucleotides)\n",
    "        self.frame = frame\n",
    "        self.name = name\n",
    "        self.score = score\n",
    "        self.evalue = evalue\n",
    "        self.startline = -1\n",
    "        self.endline = -1\n",
    "        self.sequence = \"\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"%s (ll.%s-%s): Score=%s, E=%s, Frame=%s: %s--%s\" % (self.name, self.startline, self.endline, \n",
    "                                                                    self.score, self.evalue, \n",
    "                                                                    self.frame, self.start_index, self.end_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "# # 1 # Convert blast (default out-format: extensive sequence-information contained) to gff\n",
    "seq_name = \"NODE_103_length_148054_cov_1.718544\"\n",
    "outfile = \"out/blastx_q.NODE_103_wrt_ciliates_org.gff\"\n",
    "\n",
    "with open(outfile, \"w\") as out:\n",
    "    out.write(\"\")\n",
    "\n",
    "current_hit = None\n",
    "current_hitname = \"\"\n",
    "hits = []\n",
    "\n",
    "with open(\"blastx_q.NODE_103_wrt_ciliates_org.out\") as blastx, open(outfile, \"a\") as out:\n",
    "    for row, line in enumerate(blastx):\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "            \n",
    "        if line.startswith(\">\"):\n",
    "            current_hitname = line[1:].replace(\"\\t\", \" \")[:-1]\n",
    "            \n",
    "        if line.startswith(\" Score = \"):\n",
    "            if current_hit is not None:\n",
    "                hits.append(current_hit)\n",
    "                print(current_hit)\n",
    "                gff_start = min(current_hit.start_index, current_hit.end_index)\n",
    "                gff_end = max(current_hit.start_index, current_hit.end_index)\n",
    "                out.write(\"\\t\".join([seq_name, \"blastx\", \"conserved\", \n",
    "                                     str(gff_start), str(gff_end), \n",
    "                                     str(current_hit.score), \"+\" if current_hit.frame >= 0 else \"-\",\n",
    "                                     str(abs(current_hit.frame)), \n",
    "                                     \"matched=%s; starts=%s; ends=%s\\n\" % (current_hit.name, \n",
    "                                                                           current_hit.sequence[:10],\n",
    "                                                                           current_hit.sequence[-10:])]))\n",
    "                \n",
    "            current_hit = Hit(current_hitname)\n",
    "            current_hit.startline = row\n",
    "            content = [e.strip() for e in line.split(\",\")]\n",
    "            current_hit.score = float(content[0].split(\" \")[2])\n",
    "            current_hit.evalue = float(content[1].split(\" \")[2])\n",
    "            \n",
    "        if line.startswith(\"Query \"):\n",
    "            current_hit.endline = row + 3\n",
    "            current_hit.end_index = int(line.split(\" \")[-1])\n",
    "            current_hit.sequence += line.split(\" \")[-3]\n",
    "            if current_hit.start_index is None:\n",
    "                current_hit.start_index = int(line.split(\"  \")[1])\n",
    "        \n",
    "        if line.startswith(\" Frame = \"):\n",
    "            current_hit.frame = int(line[8:])\n",
    "\n",
    "print(len(hits))\n",
    "\n",
    "\n",
    "# # 2 # Transforming output format 6 into GFF (just some reordering/relabeling)\n",
    "with open(\"ciliates/blastx_q.transcriptome_wrt_conserved_tetrahymena.out\") as blastx, \\\n",
    "     open(\"out/blastx_q.transcriptome_wrt_tetra.gff\", \"w\") as out:\n",
    "    output = \"\"\n",
    "    for line in blastx:\n",
    "        qseqid, sseqid, _, length, _, _, qstart, qend, sstart, send, evalue, bitscore = line[:-1].split(\"\\t\")\n",
    "        # Here, all hits happen to be on +strand\n",
    "        output += \"\\t\".join([qseqid, \"blastx\", \"match\", qstart, qend, bitscore, \"+\", \".\", 'matches=\"%s\"' % sseqid])\n",
    "        output += \"\\n\"\n",
    "        \n",
    "    out.write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting regions for samtools view\n",
    "\n",
    "While also collecting whither the specified transcript matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which = \"TRINITY_DN714_c0_g2_i1\" # Here select the transcript, whose hits are desired: This was used for manual annotation\n",
    "which_hit = []\n",
    "tolerance = 300 # <- This is the point where the flanking-NCS-size was decided\n",
    "regions_per_file = 18000\n",
    "breakpoints = [8000, 16000, 36000]\n",
    "n = 1\n",
    "\n",
    "with open('blastn_q.polyA_wrt_LmagMAC_draft_genome.out') as blast_result:\n",
    "    output = open('out/blastn_q.polyA_wrt_LmagMAC_dragen_%s.bed' % n, \"w\")\n",
    "    for row, line in enumerate(blast_result):\n",
    "        if row % 10000 == 0:\n",
    "            print(row)\n",
    "        # if row % regions_per_file == 0:\n",
    "        if row in breakpoints:\n",
    "            n += 1\n",
    "            output.close()\n",
    "            output = open('out/blastn_q.polyA_wrt_LmagMAC_dragen_%s.bed' % n, \"w\")\n",
    "            \n",
    "        \n",
    "        content = line[:-1].split(\"\\t\")\n",
    "        contig = content[1]\n",
    "        start = int(content[8])\n",
    "        end = int(content[9])\n",
    "        larger = max(start, end)\n",
    "        start = min(start, end)\n",
    "        end = larger\n",
    "        length = int(contig.split(\"_\")[3])\n",
    "        # print(content)\n",
    "        output.write(\"%s\\t%s\\t%s\\n\" % (contig, max(0, start - tolerance), min(length, end + tolerance)))\n",
    "        \n",
    "        if content[0] == which:\n",
    "            which_hit.append(\"%s q[%s, %s] s[%s, %s]\\t %%id=%s, #gap=%s, e=%s, score=%s\" % \n",
    "                             (content[1], content[6], content[7], content[8], content[9], \n",
    "                              content[3], content[5], content[10], content[11]))\n",
    "    output.close()\n",
    "    \n",
    "print(which, \"\\n\")\n",
    "print(\"\\n\".join(which_hit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the training-annotation into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_whitelist = [1, 6, 7, 10, 12, 18, 19, 20, 21, 22, 23, 25,\n",
    "           27, 29, 31, 38, 39, 42, 44, 54, 64, 65, 69, 70,\n",
    "           71, 75, 78, 80, 84, 90, 91, 103, 104, 114, 127,\n",
    "           132, 134, 135, 145, 150, 153, 155, 159, 160,\n",
    "           164, 176, 177, 181, 202, 206, 207, 210, 217,\n",
    "           222, 227, 244, 246, 250, 259, 262, 264, 265,\n",
    "           267, 271, 273, 278, 279, 296, 297, 301, 322, \n",
    "           324, 336, 339, 344, 345, 358, 364, 376, 379, \n",
    "           386, 389, 410, 426, 427, 450, # 455, #<- this one contains NNN!\n",
    "           469, 476, 479, 485, 487, 502, 508, 518, 521, \n",
    "           547, 555, 582, 585, 586, # 615, # <- this one has the over-long introns with little support from read-coverage\n",
    "           624, 626, 632, 641, 662, \n",
    "           663, 712, 721, 725, 726, 748, 762, 774, 810, \n",
    "           847, 889, 893, 896, 918, 926, 998, 1067, 1083, \n",
    "           1087, 1148, 1178, 1295, 1401, 1404, 1482, 1721, \n",
    "           1864, 2222, 3449, 4094, 4127, 4354, 9798, 13726]\n",
    "print(len(node_whitelist), \"contigs were have useable annotation\")\n",
    "\n",
    "node_whitelist = [i for i in node_whitelist if i > 50] \n",
    "# reduce set slightly to make computations faster (throw out longest contigs)\n",
    "print(\"->\", len(node_whitelist))\n",
    "\n",
    "# split into test and train:\n",
    "test_size = 65\n",
    "train_size = len(node_whitelist) - test_size\n",
    "training_whitelist = np.random.choice(node_whitelist, train_size, replace=False)\n",
    "test_whitelist = [i for i in node_whitelist if i not in training_whitelist]\n",
    "print(\"#training =\", len(training_whitelist), \"#testing =\", len(test_whitelist))\n",
    "print(\"training:\\n\", training_whitelist)\n",
    "print(\"testing:\\n\", test_whitelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_name = \"scaffolds.fasta\"\n",
    "outfile_name = \"out/intermediate_testing.fasta\"\n",
    "\n",
    "with open(outfile_name, \"w\") as out:\n",
    "    out.write(\"\")\n",
    "\n",
    "current_header = None\n",
    "with open(infile_name) as infile, open(outfile_name, \"a\") as out:\n",
    "    for row, line in enumerate(infile):\n",
    "        if line.startswith(\">\"):\n",
    "            if int(line.split(\"_\")[1]) in test_whitelist: # this previously was: in node_whitelist\n",
    "                current_header = line[1:-1]\n",
    "                out.write(line)\n",
    "            else:\n",
    "                current_header = None\n",
    "        elif current_header is not None:\n",
    "            out.write(line)\n",
    "            \n",
    "with open(\"scaffolds.only_training_dataset_before_september.cleaned.gff\") as full_annotation, \\\n",
    "    open(\"out/intermediate_training.gff\", \"w\") as outfile:\n",
    "    out = \"\"\n",
    "    for line in full_annotation:\n",
    "        if line.startswith(\"##sequen\"):\n",
    "            if int(line.split(\"\\t\")[1].split(\"_\")[1]) in training_whitelist:\n",
    "                out += line\n",
    "        elif line.startswith(\"N\") and int(line.split(\"\\t\")[0].split(\"_\")[1]) in training_whitelist:\n",
    "            out+= line\n",
    "        elif line.startswith(\"##\") and not line.startswith(\"##seq\"):\n",
    "            out += line\n",
    "    outfile.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
